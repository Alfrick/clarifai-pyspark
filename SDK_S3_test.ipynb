{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318c1270-0f7f-44fe-86a6-963339aa867a",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clarifai==9.9.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (9.9.3)\r\nRequirement already satisfied: pycocotools==2.0.6 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (2.0.6)\r\nRequirement already satisfied: pandas==1.3.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (1.3.5)\r\nRequirement already satisfied: tqdm>=4.65.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (4.66.1)\r\nRequirement already satisfied: pytest==7.4.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (7.4.1)\r\nRequirement already satisfied: PyYAML==6.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (6.0.1)\r\nRequirement already satisfied: schema==0.7.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (0.7.5)\r\nRequirement already satisfied: numpy==1.22.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (1.22.0)\r\nRequirement already satisfied: opencv-python==4.7.0.68 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (4.7.0.68)\r\nRequirement already satisfied: rich==13.4.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (13.4.2)\r\nRequirement already satisfied: clarifai-grpc==9.8.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (9.8.1)\r\nRequirement already satisfied: tritonclient==2.34.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (2.34.0)\r\nRequirement already satisfied: omegaconf==2.2.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai==9.9.3) (2.2.3)\r\nRequirement already satisfied: requests>=2.25.1 in /databricks/python3/lib/python3.10/site-packages (from clarifai-grpc==9.8.1->clarifai==9.9.3) (2.28.1)\r\nRequirement already satisfied: googleapis-common-protos>=1.53.0 in /databricks/python3/lib/python3.10/site-packages (from clarifai-grpc==9.8.1->clarifai==9.9.3) (1.56.4)\r\nRequirement already satisfied: grpcio>=1.44.0 in /databricks/python3/lib/python3.10/site-packages (from clarifai-grpc==9.8.1->clarifai==9.9.3) (1.48.1)\r\nRequirement already satisfied: protobuf>=3.20.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from clarifai-grpc==9.8.1->clarifai==9.9.3) (4.24.2)\r\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from omegaconf==2.2.3->clarifai==9.9.3) (4.9.3)\r\nRequirement already satisfied: pytz>=2017.3 in /databricks/python3/lib/python3.10/site-packages (from pandas==1.3.5->clarifai==9.9.3) (2022.1)\r\nRequirement already satisfied: python-dateutil>=2.7.3 in /databricks/python3/lib/python3.10/site-packages (from pandas==1.3.5->clarifai==9.9.3) (2.8.2)\r\nRequirement already satisfied: matplotlib>=2.1.0 in /databricks/python3/lib/python3.10/site-packages (from pycocotools==2.0.6->clarifai==9.9.3) (3.5.2)\r\nRequirement already satisfied: iniconfig in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from pytest==7.4.1->clarifai==9.9.3) (2.0.0)\r\nRequirement already satisfied: tomli>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from pytest==7.4.1->clarifai==9.9.3) (2.0.1)\r\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from pytest==7.4.1->clarifai==9.9.3) (1.1.3)\r\nRequirement already satisfied: pluggy<2.0,>=0.12 in /databricks/python3/lib/python3.10/site-packages (from pytest==7.4.1->clarifai==9.9.3) (1.0.0)\r\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from pytest==7.4.1->clarifai==9.9.3) (21.3)\r\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from rich==13.4.2->clarifai==9.9.3) (2.16.1)\r\nRequirement already satisfied: markdown-it-py>=2.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from rich==13.4.2->clarifai==9.9.3) (3.0.0)\r\nRequirement already satisfied: contextlib2>=0.5.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from schema==0.7.5->clarifai==9.9.3) (21.6.0)\r\nRequirement already satisfied: python-rapidjson>=0.9.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from tritonclient==2.34.0->clarifai==9.9.3) (1.12)\r\nRequirement already satisfied: six>=1.5.2 in /usr/lib/python3/dist-packages (from grpcio>=1.44.0->clarifai-grpc==9.8.1->clarifai==9.9.3) (1.16.0)\r\nRequirement already satisfied: mdurl~=0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich==13.4.2->clarifai==9.9.3) (0.1.2)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6->clarifai==9.9.3) (1.4.2)\r\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6->clarifai==9.9.3) (3.0.9)\r\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6->clarifai==9.9.3) (0.11.0)\r\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6->clarifai==9.9.3) (9.2.0)\r\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools==2.0.6->clarifai==9.9.3) (4.25.0)\r\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.25.1->clarifai-grpc==9.8.1->clarifai==9.9.3) (1.26.11)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.25.1->clarifai-grpc==9.8.1->clarifai==9.9.3) (3.3)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.25.1->clarifai-grpc==9.8.1->clarifai==9.9.3) (2022.9.14)\r\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.25.1->clarifai-grpc==9.8.1->clarifai==9.9.3) (2.0.4)\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\nRequirement already satisfied: pyspark in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (3.5.0)\r\nRequirement already satisfied: py4j==0.10.9.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install clarifai==9.9.3\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb25f83-599a-4835-a6e1-1d51d06a756c",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf==4.24.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages (4.24.2)\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\nName: protobuf\r\nVersion: 4.24.2\r\nSummary: \r\nHome-page: https://developers.google.com/protocol-buffers/\r\nAuthor: protobuf@googlegroups.com\r\nAuthor-email: protobuf@googlegroups.com\r\nLicense: 3-Clause BSD License\r\nLocation: /local_disk0/.ephemeral_nfs/envs/pythonEnv-9aa8405a-8613-4655-977f-9f791b447e0a/lib/python3.10/site-packages\r\nRequires: \r\nRequired-by: clarifai-grpc, facets-overview, googleapis-common-protos, grpcio-status\r\n"
     ]
    }
   ],
   "source": [
    "!pip install protobuf==4.24.2\n",
    "!pip show protobuf\n",
    "dbutils.library.restartPython() \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b3955e6-9088-4c3c-a0d2-8f860a01588e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Writing ClarifaiPySpark SDK functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f40014f9-5b31-4b14-8ec1-55b669b14f09",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### ClarifaiPySpark Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce874369-42f1-4323-bff0-6f886b02df3c",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "from clarifai.client.app import App\n",
    "from clarifai.client.dataset import Dataset\n",
    "from clarifai.client.input import Inputs\n",
    "from clarifai.client.user import User\n",
    "from clarifai.errors import UserError\n",
    "from clarifai_grpc.grpc.api.resources_pb2 import Input\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "from google.protobuf.struct_pb2 import Struct\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "  \"\"\"\n",
    "  Dataset class provides information about dataset of the app\n",
    "  and it inherits from the clarifai SDK Dataset class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, user_id: str = \"\", app_id: str = \"\", dataset_id: str = \"\"):\n",
    "    \"\"\"Initializes the Dataset object.\n",
    "\n",
    "    Args:\n",
    "        user_id (str): The clarifai user ID of the user.\n",
    "        app_id (str): Clarifai App ID.\n",
    "        dataset_id (str): Dataset ID of the dataset inside the clarifai App.\n",
    "\n",
    "    Example: TODO\n",
    "    \"\"\"\n",
    "    self.user = User(user_id=user_id)\n",
    "    self.app = App(app_id=app_id)\n",
    "    #Inputs object - for listannotations\n",
    "    #input_obj = User(user_id=\"user_id\").app(app_id=\"app_id\").inputs()\n",
    "    self.user_id = user_id\n",
    "    self.app_id = app_id\n",
    "    self.dataset_id = dataset_id\n",
    "    super().__init__(user_id=user_id, app_id=app_id, dataset_id=dataset_id)\n",
    "\n",
    "  def upload_dataset_from_csv(self,\n",
    "                              csv_path: str = \"\",\n",
    "                              input_type: str = 'text',\n",
    "                              csv_type: str = None,\n",
    "                              labels: bool = True,\n",
    "                              chunk_size: int = 128) -> None:\n",
    "    \"\"\"Uploads dataset to clarifai app from the csv file path.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): CSV file path of the dataset to be uploaded into clarifai App.\n",
    "        input_type (str): Input type of the dataset whether (Image, text).\n",
    "        csv_type (str): Type of the csv file contents(url, raw, filepath).\n",
    "        labels (bool): Give True if labels column present in dataset else False.\n",
    "        chunk_size (int): chunk size of parallel uploads of inputs and annotations.\n",
    "\n",
    "    Example: TODO\n",
    "\n",
    "    Note:\n",
    "        CSV file supports 'inputid', 'input', 'concepts', 'metadata', 'geopoints' columns.\n",
    "        All the data in the CSV should be in double quotes.\n",
    "        metadata should be in single quotes format. Example: \"{'key': 'value'}\"\n",
    "        geopoints should be in \"long,lat\" format.\n",
    "\n",
    "    \"\"\"\n",
    "    ### TODO: Can input column names & extract them to convert to our csv format\n",
    "    self.upload_from_csv(\n",
    "        csv_path=csv_path,\n",
    "        input_type=input_type,\n",
    "        csv_type=csv_type,\n",
    "        labels=labels,\n",
    "        chunk_size=chunk_size)\n",
    "\n",
    "  def upload_dataset_from_folder(self,\n",
    "                                 folder_path: str,\n",
    "                                 input_type: str,\n",
    "                                 labels: bool = False,\n",
    "                                 chunk_size: int = 128) -> None:\n",
    "    \"\"\"Uploads dataset from folder into clarifai app.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): folder path of the dataset to be uploaded into clarifai App.\n",
    "        input_type (str): Input type of the dataset whether (Image, text).\n",
    "        labels (bool): Give True if folder name is a label name else False.\n",
    "        chunk_size (int): chunk size of parallel uploads of inputs and annotations.\n",
    "\n",
    "    Example: TODO\n",
    "\n",
    "    Note:\n",
    "        Can provide a volume or S3 path to the folder\n",
    "        If label is true, then folder name is class name (label)\n",
    "    \"\"\"\n",
    "\n",
    "    self.upload_from_folder(\n",
    "        folder_path=folder_path, input_type=input_type, labels=labels, chunk_size=chunk_size)\n",
    "\n",
    "  def _get_inputs_from_dataframe(self,\n",
    "                                dataframe,\n",
    "                                input_type: str,\n",
    "                                df_type: str,\n",
    "                                dataset_id: str = None,\n",
    "                                labels: str = True) -> List[Input]:\n",
    "    input_protos = []\n",
    "    input_obj = Inputs(user_id=self.user_id, app_id=self.app_id)\n",
    "\n",
    "    for row in dataframe.collect():\n",
    "      if labels:\n",
    "        labels_list = row[\"concepts\"].split(',')\n",
    "        labels = labels_list if len(row['concepts']) > 0 else None\n",
    "      else:\n",
    "        labels = None\n",
    "\n",
    "      if 'metadata' in dataframe.columns:\n",
    "        if row['metadata'] is not None and len(row['metadata']) > 0:\n",
    "          metadata_str = row['metadata'].replace(\"'\", '\"')\n",
    "          try:\n",
    "            metadata_dict = json.loads(metadata_str)\n",
    "          except json.decoder.JSONDecodeError:\n",
    "            raise UserError(\"metadata column in CSV file should be a valid json\")\n",
    "          metadata = Struct()\n",
    "          metadata.update(metadata_dict)\n",
    "        else:\n",
    "          metadata = None\n",
    "      else:\n",
    "        metadata = None\n",
    "\n",
    "      if 'geopoints' in dataframe.columns:\n",
    "        if row['geopoints'] is not None and len(row['geopoints']) > 0:\n",
    "          geo_points = row['geopoints'].split(',')\n",
    "          geo_points = [float(geo_point) for geo_point in geo_points]\n",
    "          geo_info = geo_points if len(geo_points) == 2 else UserError(\n",
    "              \"geopoints column in CSV file should have longitude,latitude\")\n",
    "        else:\n",
    "          geo_info = None\n",
    "      else:\n",
    "        geo_info = None\n",
    "\n",
    "      input_id = row['inputid'] if 'inputid' in dataframe.columns else uuid.uuid4().hex\n",
    "      text = row[\"input\"] if input_type == 'text' else None\n",
    "      image = row['input'] if input_type == 'image' else None\n",
    "      video = row['input'] if input_type == 'video' else None\n",
    "      audio = row['input'] if input_type == 'audio' else None\n",
    "\n",
    "      if df_type == 'raw':\n",
    "        input_protos.append(\n",
    "            input_obj.get_text_input(\n",
    "                input_id=input_id,\n",
    "                raw_text=text,\n",
    "                dataset_id=dataset_id,\n",
    "                labels=labels,\n",
    "                geo_info=geo_info))\n",
    "      elif df_type == 'url':\n",
    "        input_protos.append(\n",
    "            input_obj.get_input_from_url(\n",
    "                input_id=input_id,\n",
    "                image_url=image,\n",
    "                text_url=text,\n",
    "                audio_url=audio,\n",
    "                video_url=video,\n",
    "                dataset_id=dataset_id,\n",
    "                labels=labels,\n",
    "                geo_info=geo_info))\n",
    "      else:\n",
    "        input_protos.append(\n",
    "            input_obj.get_input_from_file(\n",
    "                input_id=input_id,\n",
    "                image_file=image,\n",
    "                text_file=text,\n",
    "                audio_file=audio,\n",
    "                video_file=video,\n",
    "                dataset_id=dataset_id,\n",
    "                labels=labels,\n",
    "                geo_info=geo_info))\n",
    "\n",
    "    return input_protos\n",
    "\n",
    "  def upload_dataset_from_dataframe(self,\n",
    "                                    dataframe,\n",
    "                                    input_type: str,\n",
    "                                    df_type: str = None,\n",
    "                                    labels: bool = True,\n",
    "                                    chunk_size: int = 128) -> None:\n",
    "    \"\"\"Uploads dataset from a dataframe.\n",
    "       Expected columns in the dataframe are inputid, input, concepts (optional), metadata (optional), geopoints (optional).\n",
    "\n",
    "      Args:\n",
    "          task (str): task type(text_clf, visual-classification, visual_detection, visual_segmentation, visual-captioning).\n",
    "          split (str): split type(train, test, val).\n",
    "          module_dir (str): path to the module directory.\n",
    "          dataset_loader (str): name of the dataset loader.\n",
    "          chunk_size (int): chunk size for concurrent upload of inputs and annotations.\n",
    "\n",
    "      Example: TODO\n",
    "    \"\"\"\n",
    "\n",
    "    if input_type not in ('image', 'text', 'video', 'audio'):\n",
    "      raise UserError('Invalid input type, it should be image,text,audio or video')\n",
    "\n",
    "    if df_type not in ('raw', 'url', 'file_path'):\n",
    "      raise UserError('Invalid csv type, it should be raw, url or file_path')\n",
    "\n",
    "    if df_type == 'raw' and input_type != 'text':\n",
    "      raise UserError('Only text input type is supported for raw csv type')\n",
    "\n",
    "    if not isinstance(dataframe, SparkDataFrame):\n",
    "      raise UserError('dataframe should be a Spark DataFrame')\n",
    "\n",
    "    chunk_size = min(128, chunk_size)\n",
    "    input_obj = input_obj = Inputs(user_id=self.user_id, app_id=self.app_id)\n",
    "    input_protos = self._get_inputs_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        df_type=df_type,\n",
    "        input_type=input_type,\n",
    "        dataset_id=self.dataset_id,\n",
    "        labels=labels)\n",
    "    return (input_obj._bulk_upload(inputs=input_protos, chunk_size=chunk_size))\n",
    "\n",
    "  def upload_dataset_from_dataloader(self,\n",
    "                                     task: str,\n",
    "                                     split: str,\n",
    "                                     module_dir: str = None,\n",
    "                                     chunk_size: int = 128) -> None:\n",
    "    \"\"\"Uploads dataset using a dataloader function for custom formats.\n",
    "\n",
    "    Args:\n",
    "        task (str): task type(text_clf, visual-classification, visual_detection, visual_segmentation, visual-captioning).\n",
    "        split (str): split type(train, test, val).\n",
    "        module_dir (str): path to the module directory.\n",
    "        chunk_size (int): chunk size for concurrent upload of inputs and annotations.\n",
    "\n",
    "    Example: TODO\n",
    "    \"\"\"\n",
    "    self.upload_dataset(task=task, split=split, module_dir=module_dir, chunk_size=chunk_size)\n",
    "\n",
    "  def upload_dataset_from_table(self,\n",
    "                                table_path: str,\n",
    "                                input_type: str,\n",
    "                                table_type: str,\n",
    "                                labels: bool,\n",
    "                                chunk_size: int = 128) -> None:\n",
    "    \"\"\"upload dataset to clarifai app from spark tables.\n",
    "\n",
    "    Args:\n",
    "        table_path (str): path of the table to be uploaded.\n",
    "        task (str):\n",
    "        split (str):\n",
    "        input_type (str): Input type of the dataset whether (Image, text).\n",
    "        table_type (str): Type of the table contents (url, raw, filepath).\n",
    "        labels (bool): Give True if labels column present in dataset else False.\n",
    "        module_dir (str): path to the module directory.\n",
    "        dataset_loader (str): name of the dataset loader.\n",
    "        chunk_size (int): chunk size for concurrent upload of inputs and annotations.\n",
    "    Note:\n",
    "        Accepted csv format - input, label\n",
    "        TODO: dataframe dataloader template\n",
    "        TODO: Can input column names & extreact them to convert to our csv format\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder.appName('Clarifai-spark').getOrCreate()\n",
    "    tempdf = spark.read.format(\"delta\").load(table_path)\n",
    "    self.upload_from_dataframe(\n",
    "        dataframe=tempdf,\n",
    "        input_type=input_type,\n",
    "        df_type=table_type,\n",
    "        labels=labels,\n",
    "        chunk_size=chunk_size)\n",
    "\n",
    "  def list_inputs(self, per_page: int = None, input_type: str = None):\n",
    "    \"\"\"Lists all the inputs from the app.\n",
    "\n",
    "    Args:\n",
    "        per_page (str): No of response of inputs per page.\n",
    "        input_type (str): Input type that needs to be displayed (text,image)\n",
    "        TODO: Do we need input_type ?, since in our case it is image, so probably we can go with default value of \"image\".\n",
    "\n",
    "    Examples:\n",
    "        TODO\n",
    "\n",
    "    Returns:\n",
    "        list of inputs.\n",
    "        \"\"\"\n",
    "    input_obj = Inputs(user_id=self.user_id, app_id=self.app_id)\n",
    "    return input_obj.list_inputs(\n",
    "        dataset_id=self.dataset_id, input_type=input_type, per_page=per_page)\n",
    "\n",
    "  def list_annotations(self, input_ids: list = None, per_page: int = None, input_type: str = None):\n",
    "    \"\"\"Lists all the annotations for the inputs in the dataset of a clarifai app.\n",
    "\n",
    "    Args:\n",
    "        input_ids (list): list of input_ids for which user wants annotations\n",
    "        per_page (str): No of response of inputs per page.\n",
    "        input_type (str): Input type that needs to be displayed (text,image)\n",
    "        TODO: Do we need input_type ?, since in our case it is image, so probably we can go with default value of \"image\".\n",
    "\n",
    "    Examples:\n",
    "        TODO\n",
    "\n",
    "    Returns:\n",
    "        list of annotations.\n",
    "    \"\"\"\n",
    "    ### input_ids: list of input_ids for which user wants annotations\n",
    "    input_obj = Inputs(user_id=self.user_id, app_id=self.app_id)\n",
    "    if not input_ids:\n",
    "        all_inputs = list(\n",
    "            input_obj.list_inputs(\n",
    "                dataset_id=self.dataset_id, input_type=input_type, per_page=per_page))\n",
    "    else:\n",
    "        all_inputs = [input_obj._get_proto(input_id=inpid, dataset_id=self.dataset_id) for inpid in input_ids]\n",
    "    return input_obj.list_annotations(batch_input=all_inputs)\n",
    "\n",
    "  def export_annotations_to_dataframe(self, input_ids: list = None):\n",
    "    \"\"\"Export all the annotations from clarifai App's dataset to spark dataframe.\n",
    "\n",
    "    Args:\n",
    "        input_ids (list): list of input_ids for which user wants annotations\n",
    "\n",
    "    Examples:\n",
    "        TODO\n",
    "\n",
    "    Returns:\n",
    "        spark dataframe with annotations\"\"\"\n",
    "\n",
    "    annotation_list = []\n",
    "    spark = SparkSession.builder.appName('Clarifai-spark').getOrCreate()\n",
    "    response = list(self.list_annotations(input_ids=input_ids))\n",
    "    for an in response:\n",
    "      temp = {}\n",
    "      temp['annotation'] = json.loads(MessageToJson(an.data))\n",
    "      if not temp['annotation'] or temp['annotation'] == '{}' or temp['annotation'] == {}:\n",
    "        continue\n",
    "      temp['id'] = an.id\n",
    "      temp['user_id'] = an.user_id\n",
    "      temp['input_id'] = an.input_id\n",
    "      try:\n",
    "        created_at = float(f\"{an.created_at.seconds}.{an.created_at.nanos}\")\n",
    "        temp['created_at'] = time.strftime('%m/%d/% %H:%M:%5', time.gmtime(created_at))\n",
    "        modified_at = float(f\"{an.modified_at.seconds}.{an.modified_at.nanos}\")\n",
    "        temp['modified_at'] = time.strftime('%m/%d/% %H:%M:%5', time.gmtime(modified_at))\n",
    "      except:\n",
    "        temp['created_at'] = float(f\"{an.created_at.seconds}.{an.created_at.nanos}\")\n",
    "        temp['modified_at'] = float(f\"{an.modified_at.seconds}.{an.modified_at.nanos}\")\n",
    "      annotation_list.append(temp)\n",
    "    return spark.createDataFrame(annotation_list)\n",
    "\n",
    "  def export_images_to_volume(self, path, input_response):\n",
    "    for resp in input_response:\n",
    "      imgid = resp.id\n",
    "      ext = resp.data.image.image_info.format\n",
    "      url = resp.data.image.url\n",
    "      img_name = path + '/' + imgid + '.' + ext.lower()\n",
    "      headers = {\"Authorization\": self.metadata[0][1]}\n",
    "      response = requests.get(url, headers=headers)\n",
    "      with open(img_name, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "  def export_text_to_volume(self, path, input_response):\n",
    "    for resp in input_response:\n",
    "      textid = resp.id\n",
    "      url = resp.data.text.url\n",
    "      file_name = path + '/' + textid + '.txt'\n",
    "      enc = resp.data.text.text_info.encoding\n",
    "      headers = {\"Authorization\": self.metadata[0][1]}\n",
    "      response = requests.get(url, headers=headers)\n",
    "      with open(file_name, \"a\", encoding=enc) as f:\n",
    "        f.write(response.content.decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1930017-2f54-478f-bbdf-264a1f746c65",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### ClarifaiPySpark Client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67816333-aec9-4535-9af4-810115231054",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2023-10-27 16:33:56 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> INFO:py4j.clientserver:Received command c on object id p0          <a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">clientserver.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m2023-10-27 16:33:56\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m INFO:py4j.clientserver:Received command c on object id p0          \u001B]8;id=487946;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001B\\\u001B[2mclientserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=955483;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\u001B\\\u001B[2m575\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from clarifai.client.app import App\n",
    "from clarifai.client.base import BaseClient\n",
    "from clarifai.client.user import User\n",
    "\n",
    "# from clarifaipyspark.dataset import Dataset\n",
    "\n",
    "\n",
    "class ClarifaiPySpark(BaseClient):\n",
    "  \"\"\"\n",
    "  ClarifaiPySpark inherits the BaseClient class from the clarifai SDK and it initializes the client.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, user_id: str = \"\", app_id: str = \"\"):\n",
    "    \"\"\"Initializes clarifai client object.\n",
    "\n",
    "    Args:\n",
    "      - user_id (str): A user ID for authentication.\n",
    "      - app_id (str): An app ID for the application to interact with.\n",
    "    \"\"\"\n",
    "\n",
    "    self.user = User(user_id=user_id)\n",
    "    self.app = App(app_id=app_id)\n",
    "    self.user_id = user_id\n",
    "    self.app_id = app_id\n",
    "    super().__init__(user_id=user_id, app_id=app_id)\n",
    "\n",
    "  def dataset(self, dataset_id):\n",
    "    \"\"\"Initializes the dataset method with dataset_id.\n",
    "\n",
    "    Args:\n",
    "      dataset_id: The dataset_id within the user app.\n",
    "\n",
    "    Returns:\n",
    "      Dataset object for the dataset_id.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "      self.app.dataset(dataset_id=dataset_id)\n",
    "    except:\n",
    "      print(\"Creating a new dataset\")\n",
    "      self.app.create_dataset(dataset_id=dataset_id)\n",
    "\n",
    "    return Dataset(dataset_id=dataset_id, user_id=self.user_id, app_id=self.app_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e324b745-65dc-4e9b-ac56-cc307dc32e20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Testing ClarifaiPySpark SDK with S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b49e60e6-14d5-41cf-98bf-c1d3bac0e326",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Setting Env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c40e27-74cc-400c-b287-ef4946ec9f13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2023-10-27 16:34:00 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> INFO:py4j.clientserver:Received command c on object id p0          <a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">clientserver.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m2023-10-27 16:34:00\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m INFO:py4j.clientserver:Received command c on object id p0          \u001B]8;id=904373;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001B\\\u001B[2mclientserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=915338;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\u001B\\\u001B[2m575\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CLARIFAI_PAT'] = '95978ef1e65e4e1ab8b268e94a49b1e9'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1d7478c-e90a-4060-aeac-4fd9c302e147",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating ClarifaiPyspark object & creating/fetching image dataset from app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "278fbfb8-3a3c-4556-8066-f801c368f7fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2023-10-27 16:34:03 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> INFO:py4j.clientserver:Received command c on object id p0          <a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">clientserver.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m2023-10-27 16:34:03\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m INFO:py4j.clientserver:Received command c on object id p0          \u001B]8;id=188473;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001B\\\u001B[2mclientserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=589801;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\u001B\\\u001B[2m575\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cspark_obj = ClarifaiPySpark(user_id='mansi_k', app_id='databricks_tester_img')\n",
    "\n",
    "dataset_obj = cspark_obj.dataset(dataset_id='dataset1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "804197c3-da0a-4560-b75a-85f109d06c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Uploading dataset from CSV stored in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f01f5b-2dca-466f-9692-f5c0dbb13465",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2023-10-27 10:26:13 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> INFO:py4j.clientserver:Received command c on object id p0          <a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">clientserver.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m2023-10-27 10:26:13\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m INFO:py4j.clientserver:Received command c on object id p0          \u001B]8;id=472622;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001B\\\u001B[2mclientserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=531383;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\u001B\\\u001B[2m575\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+--------------------+\n|inputid|               input|concepts|            metadata|\n+-------+--------------------+--------+--------------------+\n|image01|https://samples.c...|   image|{'filename': 'can...|\n|image02|https://samples.c...|   image|                NULL|\n|image03|https://samples.c...|   image|{'filename': 'doo...|\n+-------+--------------------+--------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.format(\"csv\").option('header', 'true').load(\"s3://new-bucket-for-databricks-integration-23102023/image_urls3.csv\")\n",
    "df_csv.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33c633e-af12-47d5-b47d-a0269f516509",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2023-10-27 12:02:25 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> INFO:py4j.clientserver:Received command c on object id p0          <a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">clientserver.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m2023-10-27 12:02:25\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m INFO:py4j.clientserver:Received command c on object id p0          \u001B]8;id=801299;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001B\\\u001B[2mclientserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=464067;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\u001B\\\u001B[2m575\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rUploading inputs:   0%|          | 0/1 [00:00<?, ?it/s]\rUploading inputs: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\rUploading inputs: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_obj.upload_dataset_from_csv(csv_path='s3://new-bucket-for-databricks-integration-23102023/image_urls3.csv', source='s3', input_type='image', csv_type='url', labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efef332f-fcad-4513-895d-fa54e4d25c9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Uploading dataset from a delta table stored in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee18e603-5fca-4b32-8019-f9387ed7a581",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2023-10-27 12:43:19 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> INFO:py4j.clientserver:Received command c on object id p0          <a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">clientserver.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m2023-10-27 12:43:19\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m INFO:py4j.clientserver:Received command c on object id p0          \u001B]8;id=932598;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001B\\\u001B[2mclientserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=644619;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\u001B\\\u001B[2m575\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------+\n|inputid|               input|concepts|\n+-------+--------------------+--------+\n| img111|https://cdni.auto...|     car|\n| img211|https://cdni.auto...|     car|\n+-------+--------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df_delta = spark.read.format(\"delta\").load(\"s3://new-bucket-for-databricks-integration-23102023/img3_deltatable/\")\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c48d33-ac76-401d-b6fc-b13afbea6f42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">2023-10-27 12:48:50 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> INFO:py4j.clientserver:Received command c on object id p0          <a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">clientserver.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">575</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001B[2;36m2023-10-27 12:48:50\u001B[0m\u001B[2;36m \u001B[0m\u001B[34mINFO    \u001B[0m INFO:py4j.clientserver:Received command c on object id p0          \u001B]8;id=443884;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001B\\\u001B[2mclientserver.py\u001B[0m\u001B]8;;\u001B\\\u001B[2m:\u001B[0m\u001B]8;id=199595;file:///databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py#575\u001B\\\u001B[2m575\u001B[0m\u001B]8;;\u001B\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rUploading inputs:   0%|          | 0/1 [00:00<?, ?it/s]\rUploading inputs: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\rUploading inputs: 100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_obj.upload_dataset_from_table(table_path=\"s3://new-bucket-for-databricks-integration-23102023/img3_deltatable/\", input_type='image', table_type='url', labels=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 152087693292546,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "SDK_S3_in_book_test_27oct (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
